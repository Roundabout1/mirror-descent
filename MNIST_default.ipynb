{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmX4MaS5Yoq",
        "outputId": "12bf8e6b-a4e8-4d5b-e6ed-db920ad2c5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90853431.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 78761829.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 66468278.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18406307.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Загрузка данных MNIST\n",
        "full_train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "full_test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up2xcoYsSEZQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Разбиение обучающих и тестовых данных\n",
        "\n",
        "# Используем все тестовые данные для проверки\n",
        "test_dataset = full_test_dataset\n",
        "\n",
        "# Разбиение обучающих данных\n",
        "\n",
        "# Количество меток (от 0 до 9)\n",
        "labels_num = int(10)\n",
        "# Длина всего обучающего множества и того обучающего множества, которое мы хотим использовать для обучения\n",
        "full_train_len = len(full_train_dataset)\n",
        "# Длина того обучающего множества, которое мы хотим использовать для обучения, оно должно делиться на количество меток\n",
        "train_len = int(100)\n",
        "# Количество данных с одной меткой\n",
        "label_group_num = int(train_len/labels_num)\n",
        "\n",
        "# Создаём группы для хранения индексов каждой метки в обучающем наборе данных\n",
        "label_groups_index = [[] for _ in range(labels_num)]\n",
        "for i in range(full_train_len):\n",
        "  label = full_train_dataset[i][1]\n",
        "  label_groups_index[label].append(i)\n",
        "\n",
        "# Обрезаем группы, оставляя случайные, неповторяющиеся элементы в каждой и объединяем их всех в один набор индексов\n",
        "all_index = np.array([], dtype=int)\n",
        "for i in range(labels_num):\n",
        "  all_index = np.append(all_index, random.sample(label_groups_index[i], label_group_num))\n",
        "np.random.shuffle(all_index)\n",
        "\n",
        "# Формируем обучающий набор данных\n",
        "train_dataset = torch.utils.data.Subset(full_train_dataset, all_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ddE4F3vsubNY",
        "outputId": "5eb6f59f-72c9-4e42-bda5-8560ddf749c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1], Loss: 2.3021, Accuracy: 12.91%\n",
            "Epoch [2], Loss: 2.3033, Accuracy: 10.10%\n",
            "Epoch [3], Loss: 2.3090, Accuracy: 10.10%\n",
            "Epoch [4], Loss: 2.3095, Accuracy: 10.12%\n",
            "Epoch [5], Loss: 2.3000, Accuracy: 10.10%\n",
            "Epoch [6], Loss: 2.2998, Accuracy: 10.10%\n",
            "Epoch [7], Loss: 2.3073, Accuracy: 10.20%\n",
            "Epoch [8], Loss: 2.3038, Accuracy: 10.23%\n",
            "Epoch [9], Loss: 2.3011, Accuracy: 10.41%\n",
            "Epoch [10], Loss: 2.3039, Accuracy: 12.43%\n",
            "Epoch [11], Loss: 2.3024, Accuracy: 14.99%\n",
            "Epoch [12], Loss: 2.3025, Accuracy: 17.79%\n",
            "Epoch [13], Loss: 2.3003, Accuracy: 14.14%\n",
            "Epoch [14], Loss: 2.3042, Accuracy: 13.05%\n",
            "Epoch [15], Loss: 2.3022, Accuracy: 13.74%\n",
            "Epoch [16], Loss: 2.3015, Accuracy: 16.43%\n",
            "Epoch [17], Loss: 2.3005, Accuracy: 19.56%\n",
            "Epoch [18], Loss: 2.3027, Accuracy: 15.48%\n",
            "Epoch [19], Loss: 2.3045, Accuracy: 10.11%\n",
            "Epoch [20], Loss: 2.2997, Accuracy: 15.90%\n",
            "Epoch [21], Loss: 2.2977, Accuracy: 12.08%\n",
            "Epoch [22], Loss: 2.2984, Accuracy: 10.10%\n",
            "Epoch [23], Loss: 2.3000, Accuracy: 10.10%\n",
            "Epoch [24], Loss: 2.2980, Accuracy: 10.10%\n",
            "Epoch [25], Loss: 2.2993, Accuracy: 10.10%\n",
            "Epoch [26], Loss: 2.3012, Accuracy: 19.88%\n",
            "Epoch [27], Loss: 2.2969, Accuracy: 18.36%\n",
            "Epoch [28], Loss: 2.2962, Accuracy: 12.45%\n",
            "Epoch [29], Loss: 2.2986, Accuracy: 19.94%\n",
            "Epoch [30], Loss: 2.2955, Accuracy: 19.16%\n",
            "Epoch [31], Loss: 2.2979, Accuracy: 16.31%\n",
            "Epoch [32], Loss: 2.2952, Accuracy: 17.05%\n",
            "Epoch [33], Loss: 2.3010, Accuracy: 18.55%\n",
            "Epoch [34], Loss: 2.2952, Accuracy: 19.70%\n",
            "Epoch [35], Loss: 2.2951, Accuracy: 18.55%\n",
            "Epoch [36], Loss: 2.2958, Accuracy: 18.78%\n",
            "Epoch [37], Loss: 2.2968, Accuracy: 10.88%\n",
            "Epoch [38], Loss: 2.3012, Accuracy: 11.49%\n",
            "Epoch [39], Loss: 2.2993, Accuracy: 12.18%\n",
            "Epoch [40], Loss: 2.2990, Accuracy: 10.10%\n",
            "Epoch [41], Loss: 2.2947, Accuracy: 10.10%\n",
            "Epoch [42], Loss: 2.2959, Accuracy: 10.10%\n",
            "Epoch [43], Loss: 2.2970, Accuracy: 10.10%\n",
            "Epoch [44], Loss: 2.2956, Accuracy: 10.23%\n",
            "Epoch [45], Loss: 2.2945, Accuracy: 10.10%\n",
            "Epoch [46], Loss: 2.2954, Accuracy: 10.10%\n",
            "Epoch [47], Loss: 2.2946, Accuracy: 10.97%\n",
            "Epoch [48], Loss: 2.2959, Accuracy: 15.50%\n",
            "Epoch [49], Loss: 2.2951, Accuracy: 14.34%\n",
            "Epoch [50], Loss: 2.2945, Accuracy: 14.88%\n",
            "Epoch [51], Loss: 2.2914, Accuracy: 18.18%\n",
            "Epoch [52], Loss: 2.2935, Accuracy: 21.98%\n",
            "Epoch [53], Loss: 2.2910, Accuracy: 22.48%\n",
            "Epoch [54], Loss: 2.2887, Accuracy: 24.20%\n",
            "Epoch [55], Loss: 2.2922, Accuracy: 22.64%\n",
            "Epoch [56], Loss: 2.2899, Accuracy: 25.75%\n",
            "Epoch [57], Loss: 2.2904, Accuracy: 26.08%\n",
            "Epoch [58], Loss: 2.2934, Accuracy: 30.31%\n",
            "Epoch [59], Loss: 2.2906, Accuracy: 21.42%\n",
            "Epoch [60], Loss: 2.2886, Accuracy: 24.80%\n",
            "Epoch [61], Loss: 2.2897, Accuracy: 25.56%\n",
            "Epoch [62], Loss: 2.2893, Accuracy: 22.86%\n",
            "Epoch [63], Loss: 2.2907, Accuracy: 24.80%\n",
            "Epoch [64], Loss: 2.2886, Accuracy: 25.30%\n",
            "Epoch [65], Loss: 2.2874, Accuracy: 23.14%\n",
            "Epoch [66], Loss: 2.2845, Accuracy: 20.89%\n",
            "Epoch [67], Loss: 2.2830, Accuracy: 21.53%\n",
            "Epoch [68], Loss: 2.2892, Accuracy: 20.27%\n",
            "Epoch [69], Loss: 2.2838, Accuracy: 21.63%\n",
            "Epoch [70], Loss: 2.2860, Accuracy: 25.56%\n",
            "Epoch [71], Loss: 2.2803, Accuracy: 24.79%\n",
            "Epoch [72], Loss: 2.2853, Accuracy: 26.95%\n",
            "Epoch [73], Loss: 2.2845, Accuracy: 30.88%\n",
            "Epoch [74], Loss: 2.2785, Accuracy: 26.23%\n",
            "Epoch [75], Loss: 2.2833, Accuracy: 28.06%\n",
            "Epoch [76], Loss: 2.2794, Accuracy: 27.78%\n",
            "Epoch [77], Loss: 2.2778, Accuracy: 28.88%\n",
            "Epoch [78], Loss: 2.2773, Accuracy: 29.60%\n",
            "Epoch [79], Loss: 2.2759, Accuracy: 31.00%\n",
            "Epoch [80], Loss: 2.2711, Accuracy: 28.29%\n",
            "Epoch [81], Loss: 2.2767, Accuracy: 34.87%\n",
            "Epoch [82], Loss: 2.2676, Accuracy: 31.49%\n",
            "Epoch [83], Loss: 2.2780, Accuracy: 38.45%\n",
            "Epoch [84], Loss: 2.2794, Accuracy: 41.99%\n",
            "Epoch [85], Loss: 2.2693, Accuracy: 41.55%\n",
            "Epoch [86], Loss: 2.2699, Accuracy: 43.77%\n",
            "Epoch [87], Loss: 2.2722, Accuracy: 48.94%\n",
            "Epoch [88], Loss: 2.2626, Accuracy: 44.35%\n",
            "Epoch [89], Loss: 2.2590, Accuracy: 46.09%\n",
            "Epoch [90], Loss: 2.2560, Accuracy: 44.26%\n",
            "Epoch [91], Loss: 2.2558, Accuracy: 44.12%\n",
            "Epoch [92], Loss: 2.2576, Accuracy: 47.06%\n",
            "Epoch [93], Loss: 2.2456, Accuracy: 46.44%\n",
            "Epoch [94], Loss: 2.2611, Accuracy: 48.47%\n",
            "Epoch [95], Loss: 2.2349, Accuracy: 44.96%\n",
            "Epoch [96], Loss: 2.2517, Accuracy: 47.95%\n",
            "Epoch [97], Loss: 2.2358, Accuracy: 46.40%\n",
            "Epoch [98], Loss: 2.2394, Accuracy: 48.55%\n",
            "Epoch [99], Loss: 2.2340, Accuracy: 44.07%\n",
            "Epoch [100], Loss: 2.2348, Accuracy: 46.94%\n",
            "Epoch [101], Loss: 2.2335, Accuracy: 50.23%\n",
            "Epoch [102], Loss: 2.2231, Accuracy: 51.11%\n",
            "Epoch [103], Loss: 2.2233, Accuracy: 52.68%\n",
            "Epoch [104], Loss: 2.2113, Accuracy: 52.61%\n",
            "Epoch [105], Loss: 2.2058, Accuracy: 51.65%\n",
            "Epoch [106], Loss: 2.1775, Accuracy: 50.09%\n",
            "Epoch [107], Loss: 2.1638, Accuracy: 48.38%\n",
            "Epoch [108], Loss: 2.2021, Accuracy: 48.11%\n",
            "Epoch [109], Loss: 2.1609, Accuracy: 47.41%\n",
            "Epoch [110], Loss: 2.1104, Accuracy: 41.01%\n",
            "Epoch [111], Loss: 2.1116, Accuracy: 40.96%\n",
            "Epoch [112], Loss: 2.0951, Accuracy: 40.10%\n",
            "Epoch [113], Loss: 2.1107, Accuracy: 43.14%\n",
            "Epoch [114], Loss: 2.0822, Accuracy: 41.88%\n",
            "Epoch [115], Loss: 1.9751, Accuracy: 39.96%\n",
            "Epoch [116], Loss: 1.9108, Accuracy: 38.75%\n",
            "Epoch [117], Loss: 1.9451, Accuracy: 36.70%\n",
            "Epoch [118], Loss: 1.9121, Accuracy: 42.74%\n",
            "Epoch [119], Loss: 1.9016, Accuracy: 36.22%\n",
            "Epoch [120], Loss: 1.8267, Accuracy: 43.13%\n",
            "Epoch [121], Loss: 1.7828, Accuracy: 43.86%\n",
            "Epoch [122], Loss: 1.7989, Accuracy: 48.05%\n",
            "Epoch [123], Loss: 1.7427, Accuracy: 47.63%\n",
            "Epoch [124], Loss: 1.6285, Accuracy: 39.52%\n",
            "Epoch [125], Loss: 1.6802, Accuracy: 54.50%\n",
            "Epoch [126], Loss: 1.5203, Accuracy: 54.04%\n",
            "Epoch [127], Loss: 1.4009, Accuracy: 54.31%\n",
            "Epoch [128], Loss: 1.3108, Accuracy: 50.73%\n",
            "Epoch [129], Loss: 1.5464, Accuracy: 33.24%\n",
            "Epoch [130], Loss: 1.5770, Accuracy: 57.40%\n",
            "Epoch [131], Loss: 1.2368, Accuracy: 42.13%\n",
            "Epoch [132], Loss: 2.0916, Accuracy: 32.78%\n",
            "Epoch [133], Loss: 1.3104, Accuracy: 48.52%\n",
            "Epoch [134], Loss: 1.4009, Accuracy: 61.99%\n",
            "Epoch [135], Loss: 1.1868, Accuracy: 48.16%\n",
            "Epoch [136], Loss: 1.0410, Accuracy: 54.14%\n",
            "Epoch [137], Loss: 0.9135, Accuracy: 59.06%\n",
            "Epoch [138], Loss: 1.2349, Accuracy: 37.77%\n",
            "Epoch [139], Loss: 1.6713, Accuracy: 53.46%\n",
            "Epoch [140], Loss: 0.9357, Accuracy: 55.11%\n",
            "Epoch [141], Loss: 1.1971, Accuracy: 40.95%\n",
            "Epoch [142], Loss: 1.1519, Accuracy: 60.46%\n",
            "Epoch [143], Loss: 0.8227, Accuracy: 49.80%\n",
            "Epoch [144], Loss: 1.1213, Accuracy: 44.87%\n",
            "Epoch [145], Loss: 0.9730, Accuracy: 64.53%\n",
            "Epoch [146], Loss: 0.7528, Accuracy: 56.19%\n",
            "Epoch [147], Loss: 0.5610, Accuracy: 61.23%\n",
            "Epoch [148], Loss: 0.6617, Accuracy: 54.95%\n",
            "Epoch [149], Loss: 0.7454, Accuracy: 43.67%\n",
            "Epoch [150], Loss: 2.9124, Accuracy: 45.18%\n",
            "Epoch [151], Loss: 1.1187, Accuracy: 57.92%\n",
            "Epoch [152], Loss: 0.7610, Accuracy: 65.02%\n",
            "Epoch [153], Loss: 0.6140, Accuracy: 63.39%\n",
            "Epoch [154], Loss: 0.5939, Accuracy: 62.27%\n",
            "Epoch [155], Loss: 0.5778, Accuracy: 56.67%\n",
            "Epoch [156], Loss: 0.7782, Accuracy: 50.69%\n",
            "Epoch [157], Loss: 1.0025, Accuracy: 58.27%\n",
            "Epoch [158], Loss: 0.6779, Accuracy: 42.81%\n",
            "Epoch [159], Loss: 2.0155, Accuracy: 44.92%\n",
            "Epoch [160], Loss: 1.1270, Accuracy: 59.10%\n",
            "Epoch [161], Loss: 0.6353, Accuracy: 66.59%\n",
            "Epoch [162], Loss: 0.4526, Accuracy: 65.16%\n",
            "Epoch [163], Loss: 0.3590, Accuracy: 67.95%\n",
            "Epoch [164], Loss: 0.3321, Accuracy: 67.92%\n",
            "Epoch [165], Loss: 0.2925, Accuracy: 64.48%\n",
            "Epoch [166], Loss: 0.2464, Accuracy: 65.22%\n",
            "Epoch [167], Loss: 0.1690, Accuracy: 66.95%\n",
            "Epoch [168], Loss: 0.1412, Accuracy: 68.31%\n",
            "Epoch [169], Loss: 0.1080, Accuracy: 68.42%\n",
            "Epoch [170], Loss: 0.1293, Accuracy: 63.94%\n",
            "Epoch [171], Loss: 0.5289, Accuracy: 31.13%\n",
            "Epoch [172], Loss: 3.1015, Accuracy: 24.51%\n",
            "Epoch [173], Loss: 1.1824, Accuracy: 52.22%\n",
            "Epoch [174], Loss: 0.7712, Accuracy: 62.44%\n",
            "Epoch [175], Loss: 0.5760, Accuracy: 67.68%\n",
            "Epoch [176], Loss: 0.3476, Accuracy: 70.22%\n",
            "Epoch [177], Loss: 0.2300, Accuracy: 70.21%\n",
            "Epoch [178], Loss: 0.2373, Accuracy: 70.40%\n",
            "Epoch [179], Loss: 0.1681, Accuracy: 70.49%\n",
            "Epoch [180], Loss: 0.1128, Accuracy: 70.01%\n",
            "Epoch [181], Loss: 0.1359, Accuracy: 69.94%\n",
            "Epoch [182], Loss: 0.0852, Accuracy: 69.76%\n",
            "Epoch [183], Loss: 0.0609, Accuracy: 70.20%\n",
            "Epoch [184], Loss: 0.0646, Accuracy: 70.16%\n",
            "Epoch [185], Loss: 0.0593, Accuracy: 69.97%\n",
            "Epoch [186], Loss: 0.0612, Accuracy: 69.27%\n",
            "Epoch [187], Loss: 0.0419, Accuracy: 70.07%\n",
            "Epoch [188], Loss: 0.0359, Accuracy: 69.97%\n",
            "Epoch [189], Loss: 0.0273, Accuracy: 69.91%\n",
            "Epoch [190], Loss: 0.0316, Accuracy: 69.75%\n",
            "Epoch [191], Loss: 0.0286, Accuracy: 69.83%\n",
            "Epoch [192], Loss: 0.0314, Accuracy: 69.67%\n",
            "Epoch [193], Loss: 0.0218, Accuracy: 69.61%\n",
            "Epoch [194], Loss: 0.0236, Accuracy: 69.73%\n",
            "Epoch [195], Loss: 0.0236, Accuracy: 69.75%\n",
            "Epoch [196], Loss: 0.0185, Accuracy: 69.58%\n",
            "Epoch [197], Loss: 0.0260, Accuracy: 69.43%\n",
            "Epoch [198], Loss: 0.0138, Accuracy: 69.54%\n",
            "Epoch [199], Loss: 0.0146, Accuracy: 69.64%\n",
            "Epoch [200], Loss: 0.0160, Accuracy: 69.69%\n",
            "Epoch [201], Loss: 0.0140, Accuracy: 69.55%\n",
            "Epoch [202], Loss: 0.0121, Accuracy: 69.56%\n",
            "Epoch [203], Loss: 0.0122, Accuracy: 69.55%\n",
            "Epoch [204], Loss: 0.0096, Accuracy: 69.62%\n",
            "Epoch [205], Loss: 0.0139, Accuracy: 69.49%\n",
            "Epoch [206], Loss: 0.0085, Accuracy: 69.47%\n",
            "Epoch [207], Loss: 0.0110, Accuracy: 69.49%\n",
            "Epoch [208], Loss: 0.0097, Accuracy: 69.52%\n",
            "Epoch [209], Loss: 0.0098, Accuracy: 69.52%\n",
            "Epoch [210], Loss: 0.0074, Accuracy: 69.45%\n",
            "Epoch [211], Loss: 0.0102, Accuracy: 69.40%\n",
            "Epoch [212], Loss: 0.0091, Accuracy: 69.45%\n",
            "Epoch [213], Loss: 0.0075, Accuracy: 69.41%\n",
            "Epoch [214], Loss: 0.0096, Accuracy: 69.39%\n",
            "Epoch [215], Loss: 0.0085, Accuracy: 69.36%\n",
            "Epoch [216], Loss: 0.0075, Accuracy: 69.35%\n",
            "Epoch [217], Loss: 0.0053, Accuracy: 69.36%\n",
            "Epoch [218], Loss: 0.0068, Accuracy: 69.30%\n",
            "Epoch [219], Loss: 0.0068, Accuracy: 69.32%\n",
            "Epoch [220], Loss: 0.0069, Accuracy: 69.27%\n",
            "Epoch [221], Loss: 0.0057, Accuracy: 69.29%\n",
            "Epoch [222], Loss: 0.0065, Accuracy: 69.25%\n",
            "Epoch [223], Loss: 0.0057, Accuracy: 69.27%\n",
            "Epoch [224], Loss: 0.0058, Accuracy: 69.29%\n",
            "Epoch [225], Loss: 0.0064, Accuracy: 69.26%\n",
            "Epoch [226], Loss: 0.0071, Accuracy: 69.27%\n",
            "Epoch [227], Loss: 0.0040, Accuracy: 69.26%\n",
            "Epoch [228], Loss: 0.0029, Accuracy: 69.24%\n",
            "Epoch [229], Loss: 0.0039, Accuracy: 69.22%\n",
            "Epoch [230], Loss: 0.0052, Accuracy: 69.12%\n",
            "Epoch [231], Loss: 0.0051, Accuracy: 69.07%\n",
            "Epoch [232], Loss: 0.0062, Accuracy: 69.12%\n",
            "Epoch [233], Loss: 0.0055, Accuracy: 69.08%\n",
            "Epoch [234], Loss: 0.0049, Accuracy: 69.08%\n",
            "Epoch [235], Loss: 0.0046, Accuracy: 69.05%\n",
            "Epoch [236], Loss: 0.0052, Accuracy: 69.06%\n",
            "Epoch [237], Loss: 0.0044, Accuracy: 69.00%\n",
            "Epoch [238], Loss: 0.0047, Accuracy: 69.02%\n",
            "Epoch [239], Loss: 0.0047, Accuracy: 69.03%\n",
            "Epoch [240], Loss: 0.0040, Accuracy: 69.08%\n",
            "Epoch [241], Loss: 0.0031, Accuracy: 69.06%\n",
            "Epoch [242], Loss: 0.0027, Accuracy: 69.03%\n",
            "Epoch [243], Loss: 0.0029, Accuracy: 69.01%\n",
            "Epoch [244], Loss: 0.0031, Accuracy: 69.03%\n",
            "Epoch [245], Loss: 0.0037, Accuracy: 69.04%\n",
            "Epoch [246], Loss: 0.0037, Accuracy: 69.05%\n",
            "Epoch [247], Loss: 0.0037, Accuracy: 68.97%\n",
            "Epoch [248], Loss: 0.0042, Accuracy: 69.02%\n",
            "Epoch [249], Loss: 0.0038, Accuracy: 69.00%\n",
            "Epoch [250], Loss: 0.0043, Accuracy: 68.99%\n",
            "Epoch [251], Loss: 0.0026, Accuracy: 69.01%\n",
            "Epoch [252], Loss: 0.0029, Accuracy: 69.03%\n",
            "Epoch [253], Loss: 0.0035, Accuracy: 68.99%\n",
            "Epoch [254], Loss: 0.0031, Accuracy: 68.97%\n",
            "Epoch [255], Loss: 0.0022, Accuracy: 69.02%\n",
            "Epoch [256], Loss: 0.0031, Accuracy: 69.00%\n",
            "Epoch [257], Loss: 0.0028, Accuracy: 68.99%\n",
            "Epoch [258], Loss: 0.0028, Accuracy: 68.97%\n",
            "Epoch [259], Loss: 0.0032, Accuracy: 68.92%\n",
            "Epoch [260], Loss: 0.0035, Accuracy: 68.92%\n",
            "Epoch [261], Loss: 0.0031, Accuracy: 68.96%\n",
            "Epoch [262], Loss: 0.0028, Accuracy: 68.92%\n",
            "Epoch [263], Loss: 0.0024, Accuracy: 68.89%\n",
            "Epoch [264], Loss: 0.0024, Accuracy: 68.92%\n",
            "Epoch [265], Loss: 0.0033, Accuracy: 68.93%\n",
            "Epoch [266], Loss: 0.0026, Accuracy: 68.94%\n",
            "Epoch [267], Loss: 0.0023, Accuracy: 68.92%\n",
            "Epoch [268], Loss: 0.0025, Accuracy: 68.86%\n",
            "Epoch [269], Loss: 0.0034, Accuracy: 68.90%\n",
            "Epoch [270], Loss: 0.0018, Accuracy: 68.89%\n",
            "Epoch [271], Loss: 0.0020, Accuracy: 68.89%\n",
            "Epoch [272], Loss: 0.0026, Accuracy: 68.88%\n",
            "Epoch [273], Loss: 0.0017, Accuracy: 68.86%\n",
            "Epoch [274], Loss: 0.0022, Accuracy: 68.86%\n",
            "Epoch [275], Loss: 0.0020, Accuracy: 68.85%\n",
            "Epoch [276], Loss: 0.0023, Accuracy: 68.84%\n",
            "Epoch [277], Loss: 0.0024, Accuracy: 68.86%\n",
            "Epoch [278], Loss: 0.0018, Accuracy: 68.89%\n",
            "Epoch [279], Loss: 0.0025, Accuracy: 68.92%\n",
            "Epoch [280], Loss: 0.0018, Accuracy: 68.86%\n",
            "Epoch [281], Loss: 0.0024, Accuracy: 68.87%\n",
            "Epoch [282], Loss: 0.0019, Accuracy: 68.86%\n",
            "Epoch [283], Loss: 0.0020, Accuracy: 68.86%\n",
            "Epoch [284], Loss: 0.0021, Accuracy: 68.86%\n",
            "Epoch [285], Loss: 0.0023, Accuracy: 68.81%\n",
            "Epoch [286], Loss: 0.0019, Accuracy: 68.83%\n",
            "Epoch [287], Loss: 0.0022, Accuracy: 68.81%\n",
            "Epoch [288], Loss: 0.0023, Accuracy: 68.79%\n",
            "Epoch [289], Loss: 0.0018, Accuracy: 68.83%\n",
            "Epoch [290], Loss: 0.0018, Accuracy: 68.84%\n",
            "Epoch [291], Loss: 0.0014, Accuracy: 68.81%\n",
            "Epoch [292], Loss: 0.0017, Accuracy: 68.80%\n",
            "Epoch [293], Loss: 0.0022, Accuracy: 68.81%\n",
            "Epoch [294], Loss: 0.0020, Accuracy: 68.78%\n",
            "Epoch [295], Loss: 0.0021, Accuracy: 68.79%\n",
            "Epoch [296], Loss: 0.0020, Accuracy: 68.80%\n",
            "Epoch [297], Loss: 0.0018, Accuracy: 68.80%\n",
            "Epoch [298], Loss: 0.0016, Accuracy: 68.82%\n",
            "Epoch [299], Loss: 0.0019, Accuracy: 68.80%\n",
            "Epoch [300], Loss: 0.0013, Accuracy: 68.76%\n",
            "Epoch [301], Loss: 0.0018, Accuracy: 68.80%\n",
            "Epoch [302], Loss: 0.0015, Accuracy: 68.78%\n",
            "Epoch [303], Loss: 0.0016, Accuracy: 68.76%\n",
            "Epoch [304], Loss: 0.0014, Accuracy: 68.75%\n",
            "Epoch [305], Loss: 0.0017, Accuracy: 68.80%\n",
            "Epoch [306], Loss: 0.0020, Accuracy: 68.80%\n",
            "Epoch [307], Loss: 0.0017, Accuracy: 68.78%\n",
            "Epoch [308], Loss: 0.0017, Accuracy: 68.77%\n",
            "Epoch [309], Loss: 0.0016, Accuracy: 68.79%\n",
            "Epoch [310], Loss: 0.0016, Accuracy: 68.77%\n",
            "Epoch [311], Loss: 0.0016, Accuracy: 68.78%\n",
            "Epoch [312], Loss: 0.0014, Accuracy: 68.75%\n",
            "Epoch [313], Loss: 0.0014, Accuracy: 68.77%\n",
            "Epoch [314], Loss: 0.0013, Accuracy: 68.77%\n",
            "Epoch [315], Loss: 0.0015, Accuracy: 68.77%\n",
            "Epoch [316], Loss: 0.0017, Accuracy: 68.76%\n",
            "Epoch [317], Loss: 0.0013, Accuracy: 68.76%\n",
            "Epoch [318], Loss: 0.0017, Accuracy: 68.76%\n",
            "Epoch [319], Loss: 0.0014, Accuracy: 68.75%\n",
            "Epoch [320], Loss: 0.0015, Accuracy: 68.76%\n",
            "Epoch [321], Loss: 0.0015, Accuracy: 68.75%\n",
            "Epoch [322], Loss: 0.0017, Accuracy: 68.73%\n",
            "Epoch [323], Loss: 0.0013, Accuracy: 68.72%\n",
            "Epoch [324], Loss: 0.0012, Accuracy: 68.73%\n",
            "Epoch [325], Loss: 0.0015, Accuracy: 68.72%\n",
            "Epoch [326], Loss: 0.0013, Accuracy: 68.71%\n",
            "Epoch [327], Loss: 0.0013, Accuracy: 68.71%\n",
            "Epoch [328], Loss: 0.0011, Accuracy: 68.69%\n",
            "Epoch [329], Loss: 0.0014, Accuracy: 68.68%\n",
            "Epoch [330], Loss: 0.0010, Accuracy: 68.70%\n",
            "Epoch [331], Loss: 0.0013, Accuracy: 68.68%\n",
            "Epoch [332], Loss: 0.0014, Accuracy: 68.71%\n",
            "Epoch [333], Loss: 0.0012, Accuracy: 68.73%\n",
            "Epoch [334], Loss: 0.0012, Accuracy: 68.68%\n",
            "Epoch [335], Loss: 0.0010, Accuracy: 68.67%\n",
            "Epoch [336], Loss: 0.0012, Accuracy: 68.67%\n",
            "Epoch [337], Loss: 0.0012, Accuracy: 68.69%\n",
            "Epoch [338], Loss: 0.0013, Accuracy: 68.68%\n",
            "Epoch [339], Loss: 0.0014, Accuracy: 68.67%\n",
            "Epoch [340], Loss: 0.0011, Accuracy: 68.69%\n",
            "Epoch [341], Loss: 0.0014, Accuracy: 68.67%\n",
            "Epoch [342], Loss: 0.0010, Accuracy: 68.68%\n",
            "Epoch [343], Loss: 0.0012, Accuracy: 68.72%\n",
            "Epoch [344], Loss: 0.0012, Accuracy: 68.69%\n",
            "Epoch [345], Loss: 0.0012, Accuracy: 68.67%\n",
            "Epoch [346], Loss: 0.0009, Accuracy: 68.66%\n",
            "Epoch [347], Loss: 0.0009, Accuracy: 68.69%\n",
            "Epoch [348], Loss: 0.0013, Accuracy: 68.66%\n",
            "Epoch [349], Loss: 0.0009, Accuracy: 68.67%\n",
            "Epoch [350], Loss: 0.0010, Accuracy: 68.70%\n",
            "Epoch [351], Loss: 0.0012, Accuracy: 68.68%\n",
            "Epoch [352], Loss: 0.0012, Accuracy: 68.69%\n",
            "Epoch [353], Loss: 0.0011, Accuracy: 68.67%\n",
            "Epoch [354], Loss: 0.0015, Accuracy: 68.70%\n",
            "Epoch [355], Loss: 0.0010, Accuracy: 68.70%\n",
            "Epoch [356], Loss: 0.0010, Accuracy: 68.66%\n",
            "Epoch [357], Loss: 0.0011, Accuracy: 68.67%\n",
            "Epoch [358], Loss: 0.0012, Accuracy: 68.68%\n",
            "Epoch [359], Loss: 0.0009, Accuracy: 68.67%\n",
            "Epoch [360], Loss: 0.0009, Accuracy: 68.66%\n",
            "Epoch [361], Loss: 0.0012, Accuracy: 68.63%\n",
            "Epoch [362], Loss: 0.0010, Accuracy: 68.63%\n",
            "Epoch [363], Loss: 0.0011, Accuracy: 68.65%\n",
            "Epoch [364], Loss: 0.0013, Accuracy: 68.65%\n",
            "Epoch [365], Loss: 0.0010, Accuracy: 68.68%\n",
            "Epoch [366], Loss: 0.0010, Accuracy: 68.62%\n",
            "Epoch [367], Loss: 0.0009, Accuracy: 68.63%\n",
            "Epoch [368], Loss: 0.0011, Accuracy: 68.65%\n",
            "Epoch [369], Loss: 0.0011, Accuracy: 68.65%\n",
            "Epoch [370], Loss: 0.0012, Accuracy: 68.65%\n",
            "Epoch [371], Loss: 0.0010, Accuracy: 68.67%\n",
            "Epoch [372], Loss: 0.0007, Accuracy: 68.63%\n",
            "Epoch [373], Loss: 0.0009, Accuracy: 68.66%\n",
            "Epoch [374], Loss: 0.0010, Accuracy: 68.66%\n",
            "Epoch [375], Loss: 0.0007, Accuracy: 68.67%\n",
            "Epoch [376], Loss: 0.0011, Accuracy: 68.62%\n",
            "Epoch [377], Loss: 0.0008, Accuracy: 68.64%\n",
            "Epoch [378], Loss: 0.0011, Accuracy: 68.62%\n",
            "Epoch [379], Loss: 0.0007, Accuracy: 68.64%\n",
            "Epoch [380], Loss: 0.0008, Accuracy: 68.64%\n",
            "Epoch [381], Loss: 0.0007, Accuracy: 68.65%\n",
            "Epoch [382], Loss: 0.0012, Accuracy: 68.62%\n",
            "Epoch [383], Loss: 0.0011, Accuracy: 68.66%\n",
            "Epoch [384], Loss: 0.0007, Accuracy: 68.66%\n",
            "Epoch [385], Loss: 0.0007, Accuracy: 68.66%\n",
            "Epoch [386], Loss: 0.0007, Accuracy: 68.63%\n",
            "Epoch [387], Loss: 0.0007, Accuracy: 68.64%\n",
            "Epoch [388], Loss: 0.0009, Accuracy: 68.63%\n",
            "Epoch [389], Loss: 0.0009, Accuracy: 68.64%\n",
            "Epoch [390], Loss: 0.0010, Accuracy: 68.63%\n",
            "Epoch [391], Loss: 0.0007, Accuracy: 68.64%\n",
            "Epoch [392], Loss: 0.0010, Accuracy: 68.65%\n",
            "Epoch [393], Loss: 0.0010, Accuracy: 68.66%\n",
            "Epoch [394], Loss: 0.0008, Accuracy: 68.66%\n",
            "Epoch [395], Loss: 0.0009, Accuracy: 68.66%\n",
            "Epoch [396], Loss: 0.0008, Accuracy: 68.67%\n",
            "Epoch [397], Loss: 0.0009, Accuracy: 68.63%\n",
            "Epoch [398], Loss: 0.0009, Accuracy: 68.62%\n",
            "Epoch [399], Loss: 0.0009, Accuracy: 68.63%\n",
            "Epoch [400], Loss: 0.0008, Accuracy: 68.64%\n",
            "Epoch [401], Loss: 0.0007, Accuracy: 68.62%\n",
            "Epoch [402], Loss: 0.0009, Accuracy: 68.60%\n",
            "Epoch [403], Loss: 0.0007, Accuracy: 68.60%\n",
            "Epoch [404], Loss: 0.0006, Accuracy: 68.60%\n",
            "Epoch [405], Loss: 0.0008, Accuracy: 68.64%\n",
            "Epoch [406], Loss: 0.0008, Accuracy: 68.60%\n",
            "Epoch [407], Loss: 0.0009, Accuracy: 68.62%\n",
            "Epoch [408], Loss: 0.0007, Accuracy: 68.62%\n",
            "Epoch [409], Loss: 0.0008, Accuracy: 68.60%\n",
            "Epoch [410], Loss: 0.0008, Accuracy: 68.64%\n",
            "Epoch [411], Loss: 0.0008, Accuracy: 68.66%\n",
            "Epoch [412], Loss: 0.0007, Accuracy: 68.63%\n",
            "Epoch [413], Loss: 0.0008, Accuracy: 68.60%\n",
            "Epoch [414], Loss: 0.0008, Accuracy: 68.62%\n",
            "Epoch [415], Loss: 0.0007, Accuracy: 68.60%\n",
            "Epoch [416], Loss: 0.0005, Accuracy: 68.61%\n",
            "Epoch [417], Loss: 0.0008, Accuracy: 68.62%\n",
            "Epoch [418], Loss: 0.0007, Accuracy: 68.61%\n",
            "Epoch [419], Loss: 0.0006, Accuracy: 68.63%\n",
            "Epoch [420], Loss: 0.0007, Accuracy: 68.64%\n",
            "Epoch [421], Loss: 0.0006, Accuracy: 68.64%\n",
            "Epoch [422], Loss: 0.0009, Accuracy: 68.63%\n",
            "Epoch [423], Loss: 0.0006, Accuracy: 68.63%\n",
            "Epoch [424], Loss: 0.0009, Accuracy: 68.62%\n",
            "Epoch [425], Loss: 0.0006, Accuracy: 68.62%\n",
            "Epoch [426], Loss: 0.0006, Accuracy: 68.62%\n",
            "Epoch [427], Loss: 0.0007, Accuracy: 68.62%\n",
            "Epoch [428], Loss: 0.0006, Accuracy: 68.63%\n",
            "Epoch [429], Loss: 0.0008, Accuracy: 68.59%\n",
            "Epoch [430], Loss: 0.0006, Accuracy: 68.60%\n",
            "Epoch [431], Loss: 0.0006, Accuracy: 68.59%\n",
            "Epoch [432], Loss: 0.0006, Accuracy: 68.58%\n",
            "Epoch [433], Loss: 0.0004, Accuracy: 68.60%\n",
            "Epoch [434], Loss: 0.0005, Accuracy: 68.60%\n",
            "Epoch [435], Loss: 0.0006, Accuracy: 68.62%\n",
            "Epoch [436], Loss: 0.0006, Accuracy: 68.63%\n",
            "Epoch [437], Loss: 0.0006, Accuracy: 68.61%\n",
            "Epoch [438], Loss: 0.0007, Accuracy: 68.62%\n",
            "Epoch [439], Loss: 0.0007, Accuracy: 68.62%\n",
            "Epoch [440], Loss: 0.0007, Accuracy: 68.61%\n",
            "Epoch [441], Loss: 0.0007, Accuracy: 68.60%\n",
            "Epoch [442], Loss: 0.0006, Accuracy: 68.59%\n",
            "Epoch [443], Loss: 0.0007, Accuracy: 68.59%\n",
            "Epoch [444], Loss: 0.0006, Accuracy: 68.57%\n",
            "Epoch [445], Loss: 0.0008, Accuracy: 68.55%\n",
            "Epoch [446], Loss: 0.0005, Accuracy: 68.55%\n",
            "Epoch [447], Loss: 0.0007, Accuracy: 68.56%\n",
            "Epoch [448], Loss: 0.0006, Accuracy: 68.56%\n",
            "Epoch [449], Loss: 0.0005, Accuracy: 68.58%\n",
            "Epoch [450], Loss: 0.0006, Accuracy: 68.56%\n",
            "Epoch [451], Loss: 0.0005, Accuracy: 68.56%\n",
            "Epoch [452], Loss: 0.0006, Accuracy: 68.57%\n",
            "Epoch [453], Loss: 0.0006, Accuracy: 68.56%\n",
            "Epoch [454], Loss: 0.0007, Accuracy: 68.57%\n",
            "Epoch [455], Loss: 0.0007, Accuracy: 68.57%\n",
            "Epoch [456], Loss: 0.0005, Accuracy: 68.57%\n",
            "Epoch [457], Loss: 0.0004, Accuracy: 68.58%\n",
            "Epoch [458], Loss: 0.0007, Accuracy: 68.54%\n",
            "Epoch [459], Loss: 0.0006, Accuracy: 68.56%\n",
            "Epoch [460], Loss: 0.0005, Accuracy: 68.56%\n",
            "Epoch [461], Loss: 0.0007, Accuracy: 68.57%\n",
            "Epoch [462], Loss: 0.0006, Accuracy: 68.60%\n",
            "Epoch [463], Loss: 0.0007, Accuracy: 68.56%\n",
            "Epoch [464], Loss: 0.0007, Accuracy: 68.56%\n",
            "Epoch [465], Loss: 0.0005, Accuracy: 68.57%\n",
            "Epoch [466], Loss: 0.0005, Accuracy: 68.55%\n",
            "Epoch [467], Loss: 0.0006, Accuracy: 68.57%\n",
            "Epoch [468], Loss: 0.0006, Accuracy: 68.56%\n",
            "Epoch [469], Loss: 0.0004, Accuracy: 68.55%\n",
            "Epoch [470], Loss: 0.0006, Accuracy: 68.54%\n",
            "Epoch [471], Loss: 0.0006, Accuracy: 68.54%\n",
            "Epoch [472], Loss: 0.0007, Accuracy: 68.54%\n",
            "Epoch [473], Loss: 0.0006, Accuracy: 68.54%\n",
            "Epoch [474], Loss: 0.0006, Accuracy: 68.54%\n",
            "Epoch [475], Loss: 0.0007, Accuracy: 68.54%\n",
            "Epoch [476], Loss: 0.0005, Accuracy: 68.54%\n",
            "Epoch [477], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [478], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [479], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [480], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [481], Loss: 0.0006, Accuracy: 68.52%\n",
            "Epoch [482], Loss: 0.0004, Accuracy: 68.53%\n",
            "Epoch [483], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [484], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [485], Loss: 0.0006, Accuracy: 68.52%\n",
            "Epoch [486], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [487], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [488], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [489], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [490], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [491], Loss: 0.0006, Accuracy: 68.51%\n",
            "Epoch [492], Loss: 0.0006, Accuracy: 68.53%\n",
            "Epoch [493], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [494], Loss: 0.0006, Accuracy: 68.50%\n",
            "Epoch [495], Loss: 0.0004, Accuracy: 68.50%\n",
            "Epoch [496], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [497], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [498], Loss: 0.0006, Accuracy: 68.52%\n",
            "Epoch [499], Loss: 0.0005, Accuracy: 68.50%\n",
            "Epoch [500], Loss: 0.0005, Accuracy: 68.50%\n",
            "Epoch [501], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [502], Loss: 0.0007, Accuracy: 68.51%\n",
            "Epoch [503], Loss: 0.0006, Accuracy: 68.52%\n",
            "Epoch [504], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [505], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [506], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [507], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [508], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [509], Loss: 0.0006, Accuracy: 68.52%\n",
            "Epoch [510], Loss: 0.0004, Accuracy: 68.53%\n",
            "Epoch [511], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [512], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [513], Loss: 0.0005, Accuracy: 68.53%\n",
            "Epoch [514], Loss: 0.0006, Accuracy: 68.55%\n",
            "Epoch [515], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [516], Loss: 0.0006, Accuracy: 68.51%\n",
            "Epoch [517], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [518], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [519], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [520], Loss: 0.0006, Accuracy: 68.51%\n",
            "Epoch [521], Loss: 0.0005, Accuracy: 68.51%\n",
            "Epoch [522], Loss: 0.0006, Accuracy: 68.50%\n",
            "Epoch [523], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [524], Loss: 0.0005, Accuracy: 68.50%\n",
            "Epoch [525], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [526], Loss: 0.0005, Accuracy: 68.54%\n",
            "Epoch [527], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [528], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [529], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [530], Loss: 0.0005, Accuracy: 68.52%\n",
            "Epoch [531], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [532], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [533], Loss: 0.0004, Accuracy: 68.52%\n",
            "Epoch [534], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [535], Loss: 0.0004, Accuracy: 68.50%\n",
            "Epoch [536], Loss: 0.0004, Accuracy: 68.51%\n",
            "Epoch [537], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [538], Loss: 0.0006, Accuracy: 68.49%\n",
            "Epoch [539], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [540], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [541], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [542], Loss: 0.0005, Accuracy: 68.47%\n",
            "Epoch [543], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [544], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [545], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [546], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [547], Loss: 0.0004, Accuracy: 68.50%\n",
            "Epoch [548], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [549], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [550], Loss: 0.0005, Accuracy: 68.47%\n",
            "Epoch [551], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [552], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [553], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [554], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [555], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [556], Loss: 0.0004, Accuracy: 68.49%\n",
            "Epoch [557], Loss: 0.0005, Accuracy: 68.48%\n",
            "Epoch [558], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [559], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [560], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [561], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [562], Loss: 0.0005, Accuracy: 68.47%\n",
            "Epoch [563], Loss: 0.0004, Accuracy: 68.46%\n",
            "Epoch [564], Loss: 0.0004, Accuracy: 68.45%\n",
            "Epoch [565], Loss: 0.0004, Accuracy: 68.45%\n",
            "Epoch [566], Loss: 0.0004, Accuracy: 68.46%\n",
            "Epoch [567], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [568], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [569], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [570], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [571], Loss: 0.0005, Accuracy: 68.48%\n",
            "Epoch [572], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [573], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [574], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [575], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [576], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [577], Loss: 0.0003, Accuracy: 68.48%\n",
            "Epoch [578], Loss: 0.0004, Accuracy: 68.48%\n",
            "Epoch [579], Loss: 0.0005, Accuracy: 68.49%\n",
            "Epoch [580], Loss: 0.0003, Accuracy: 68.47%\n",
            "Epoch [581], Loss: 0.0003, Accuracy: 68.46%\n",
            "Epoch [582], Loss: 0.0004, Accuracy: 68.45%\n",
            "Epoch [583], Loss: 0.0004, Accuracy: 68.47%\n",
            "Epoch [584], Loss: 0.0004, Accuracy: 68.46%\n",
            "Epoch [585], Loss: 0.0004, Accuracy: 68.45%\n",
            "Epoch [586], Loss: 0.0003, Accuracy: 68.47%\n",
            "Epoch [587], Loss: 0.0003, Accuracy: 68.47%\n",
            "Epoch [588], Loss: 0.0003, Accuracy: 68.47%\n",
            "Epoch [589], Loss: 0.0003, Accuracy: 68.47%\n",
            "Epoch [590], Loss: 0.0004, Accuracy: 68.46%\n",
            "Epoch [591], Loss: 0.0004, Accuracy: 68.45%\n",
            "Epoch [592], Loss: 0.0003, Accuracy: 68.46%\n",
            "Epoch [593], Loss: 0.0004, Accuracy: 68.46%\n",
            "Epoch [594], Loss: 0.0004, Accuracy: 68.45%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-69142f7ce83b>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-69142f7ce83b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Параметры обучения\n",
        "batch_size = 64\n",
        "learning_rate = 0.1\n",
        "epochs = 5\n",
        "\n",
        "# Определение модели\n",
        "# neurons_num - количество нейронов в каждом слое\n",
        "# img_size - размер изображений из MNIST\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, neurons_num, img_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.fc1 = nn.Linear(img_size, neurons_num)\n",
        "        self.fc2 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc3 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc4 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc5 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc6 = nn.Linear(neurons_num, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.img_size)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = nn.functional.relu(self.fc3(x))\n",
        "        x = nn.functional.relu(self.fc4(x))\n",
        "        x = nn.functional.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Инициализация модели и оптимизатора\n",
        "model = Net(1000, 28*28)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Инициализация загрузчиков данных\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_array = []\n",
        "accuracy_array = []\n",
        "\n",
        "# Обучение модели\n",
        "#for epoch in range(epochs):\n",
        "epoch = 0\n",
        "while True:\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = nn.functional.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Проверка точности модели на тестовом наборе данных\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    loss_array.append(loss.item())\n",
        "    accuracy_array.append(accuracy)\n",
        "    print('Epoch [{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, loss.item(), accuracy))\n",
        "    epoch+=1\n",
        "    if accuracy >= 88.5:\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx1w09TZPXOY",
        "outputId": "c32767db-d29a-4295-a984-d17f5d3cdb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.3109, Accuracy: 93.68%\n",
            "Epoch [2/10], Loss: 0.0239, Accuracy: 96.76%\n",
            "Epoch [3/10], Loss: 0.0804, Accuracy: 96.81%\n",
            "Epoch [4/10], Loss: 0.0042, Accuracy: 97.54%\n",
            "Epoch [5/10], Loss: 0.0829, Accuracy: 96.98%\n",
            "Epoch [6/10], Loss: 0.1174, Accuracy: 94.79%\n",
            "Epoch [7/10], Loss: 0.0011, Accuracy: 97.85%\n",
            "Epoch [8/10], Loss: 0.0002, Accuracy: 98.23%\n",
            "Epoch [9/10], Loss: 0.0200, Accuracy: 97.69%\n",
            "Epoch [10/10], Loss: 0.0001, Accuracy: 98.15%\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, epochs, loss_array[epoch], accuracy_array[epoch]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
