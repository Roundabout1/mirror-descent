{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmX4MaS5Yoq",
        "outputId": "12bf8e6b-a4e8-4d5b-e6ed-db920ad2c5fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "OUTPUT_ROOT = \"output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "up2xcoYsSEZQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Разбиение обучающих данных\n",
        "# labels_num -  количество меток (от 0 до 9)\n",
        "# train_len - длина того обучающего множества, которое мы хотим использовать для обучения, оно должно делиться на количество меток\n",
        "def balancing(full_train_dataset, lables_num, train_len):\n",
        "  # Длина всего обучающего множества и того обучающего множества, которое мы хотим использовать для обучения\n",
        "  full_train_len = len(full_train_dataset)\n",
        "  # Количество данных с одной меткой\n",
        "  label_group_num = int(train_len/lables_num)\n",
        "\n",
        "  # Создаём группы для хранения индексов каждой метки в обучающем наборе данных\n",
        "  label_groups_index = [[] for _ in range(lables_num)]\n",
        "  for i in range(full_train_len):\n",
        "    label = full_train_dataset[i][1]\n",
        "    label_groups_index[label].append(i)\n",
        "\n",
        "  # Обрезаем группы, оставляя случайные, неповторяющиеся элементы в каждой и объединяем их всех в один набор индексов\n",
        "  all_index = np.array([], dtype=int)\n",
        "  for i in range(lables_num):\n",
        "    all_index = np.append(all_index, random.sample(label_groups_index[i], label_group_num))\n",
        "  np.random.shuffle(all_index)\n",
        "\n",
        "  # Формируем обучающий набор данных\n",
        "  train_dataset = torch.utils.data.Subset(full_train_dataset, all_index)\n",
        "  return train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Определение модели\n",
        "# neurons_num - количество нейронов в каждом слое\n",
        "# img_size - размер изображений из MNIST\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, neurons_num, img_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.neurons_num = neurons_num\n",
        "        self.img_size = img_size\n",
        "        self.fc1 = nn.Linear(img_size, neurons_num)\n",
        "        self.fc2 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc3 = nn.Linear(neurons_num, neurons_num)\n",
        "        self.fc4 = nn.Linear(neurons_num, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.img_size)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = nn.functional.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "    def info(self):\n",
        "        layers = 'layers: 4\\n'\n",
        "        neurons = f'neurons_num: {self.neurons_num}\\n'\n",
        "        img = f'img_size: {self.img_size}\\n'\n",
        "        active_fun = 'activation function: relu'\n",
        "        return layers + neurons + img + active_fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# обучение в течение одной эпохи\n",
        "def train_step(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Ошибка предсказания\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # вывод текущего прогресса, для того, чтобы убедиться, что обучение идёт\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    return 100*correct, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "# обучение модели и получение результатов обучения на тестовой и обучающих выборках\n",
        "def train(model, epochs, train_dataloader, test_dataloader, optimizer, loss_fn, output_path):\n",
        "    train_results = []\n",
        "    test_results = []\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        startTime = time.time()\n",
        "        train_step(train_dataloader, model, loss_fn, optimizer)\n",
        "        print('learning took {:.2f} s'.format(time.time() - startTime))\n",
        "        train_accuracy, train_loss = test(train_dataloader, model, loss_fn)\n",
        "        train_results.append((train_accuracy, train_loss))\n",
        "        test_accuracy, test_loss = test(test_dataloader, model, loss_fn)\n",
        "        test_results.append((test_accuracy, test_loss))\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    np.savetxt(os.path.join(OUTPUT_ROOT, 'train.txt'), np.array(train_results), fmt='%.1f %.8f')\n",
        "    np.savetxt(os.path.join(OUTPUT_ROOT, 'test.txt'), np.array(test_results), fmt='%.1f %.8f')\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка данных MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ddE4F3vsubNY",
        "outputId": "5eb6f59f-72c9-4e42-bda5-8560ddf749c4"
      },
      "outputs": [],
      "source": [
        "train_dataset = balancing(train_dataset, 10, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Инициализация модели \n",
        "model = Net(800, 28*28).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304408  [   64/ 1000]\n",
            "learning took 0.25 s\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.285063  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.256618  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.198928  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.050682  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.664004  [   64/ 1000]\n",
            "learning took 0.36 s\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.062984  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.741294  [   64/ 1000]\n",
            "learning took 0.28 s\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.579225  [   64/ 1000]\n",
            "learning took 0.29 s\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.467049  [   64/ 1000]\n",
            "learning took 0.23 s\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.412812  [   64/ 1000]\n",
            "learning took 0.24 s\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.329982  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.283240  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.268700  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.220671  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.192995  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.163953  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.139209  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.125766  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.111954  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.099978  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.076838  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.060063  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.054122  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.046633  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.040883  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.036720  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.033093  [   64/ 1000]\n",
            "learning took 0.21 s\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.029164  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.026229  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.023505  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.021196  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.019008  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.017110  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.015244  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.013600  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.012248  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.011092  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.010130  [   64/ 1000]\n",
            "learning took 0.23 s\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.009337  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.008671  [   64/ 1000]\n",
            "learning took 0.23 s\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.008124  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.007642  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.007219  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.006857  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.006518  [   64/ 1000]\n",
            "learning took 0.21 s\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.006210  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.005941  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.005658  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.005431  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.005197  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.004980  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.004771  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.004577  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.004393  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.004219  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.004053  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.003894  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.003745  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.003610  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.003472  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.003348  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.003230  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.003119  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.003012  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.002912  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.002817  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.002728  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.002644  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.002563  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.002487  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.002415  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.002346  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.002280  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.002219  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.002158  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.002102  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.002047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.001996  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.001945  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.001899  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.001854  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.001810  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.001767  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.001728  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.001689  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.001652  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.001616  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.001582  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.001549  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.001517  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.001486  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.001457  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.001429  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.001400  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.001374  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.001349  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.001323  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.001299  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.001276  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 0.001254  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 0.001231  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 0.001210  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 0.001190  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 0.001170  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 0.001150  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 0.001132  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 0.001114  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 0.001096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 0.001079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 0.001062  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 0.001046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 0.001030  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 0.001015  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 0.001000  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 0.000985  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 0.000971  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 0.000957  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 0.000943  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 0.000931  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 0.000917  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 0.000905  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 0.000893  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 0.000881  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 0.000869  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 0.000858  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 0.000847  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 0.000836  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 0.000825  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 0.000815  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 0.000805  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 0.000795  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 0.000785  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 0.000776  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 0.000767  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 0.000757  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 0.000749  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 0.000740  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 0.000731  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 0.000723  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 0.000715  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 0.000707  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 0.000699  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 0.000691  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 0.000683  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 0.000676  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 0.000669  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 0.000662  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 0.000655  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 0.000648  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 0.000641  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 0.000635  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 0.000628  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 0.000622  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 0.000615  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 0.000609  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 0.000603  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 0.000597  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 0.000592  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 0.000586  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 0.000580  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 0.000575  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 0.000569  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 0.000564  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 0.000559  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 0.000553  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 0.000548  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 0.000543  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 0.000538  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 0.000534  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 0.000529  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 0.000524  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 0.000520  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 0.000515  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 0.000511  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 0.000506  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 0.000502  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 0.000498  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 0.000493  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 0.000489  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 0.000485  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 0.000481  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 0.000477  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 0.000473  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 0.000470  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 0.000466  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 0.000462  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 0.000458  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 0.000455  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 0.000451  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 0.000448  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 0.000444  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 0.000441  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 0.000437  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 0.000434  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 0.000431  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 0.000428  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 0.000424  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss: 0.000421  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "loss: 0.000418  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "loss: 0.000415  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "loss: 0.000412  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "loss: 0.000409  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "loss: 0.000406  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "loss: 0.000403  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "loss: 0.000400  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "loss: 0.000397  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "loss: 0.000395  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "loss: 0.000392  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "loss: 0.000389  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "loss: 0.000386  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "loss: 0.000384  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "loss: 0.000381  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "loss: 0.000378  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "loss: 0.000376  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "loss: 0.000373  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "loss: 0.000371  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "loss: 0.000368  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "loss: 0.000366  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "loss: 0.000363  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "loss: 0.000361  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "loss: 0.000359  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "loss: 0.000356  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "loss: 0.000354  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "loss: 0.000352  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "loss: 0.000350  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "loss: 0.000347  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "loss: 0.000345  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "loss: 0.000343  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "loss: 0.000341  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "loss: 0.000339  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "loss: 0.000337  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "loss: 0.000334  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "loss: 0.000332  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "loss: 0.000330  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "loss: 0.000328  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "loss: 0.000326  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "loss: 0.000324  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "loss: 0.000322  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "loss: 0.000320  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "loss: 0.000319  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "loss: 0.000317  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "loss: 0.000315  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "loss: 0.000313  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "loss: 0.000311  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "loss: 0.000309  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "loss: 0.000308  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "loss: 0.000306  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "loss: 0.000304  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "loss: 0.000302  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "loss: 0.000301  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "loss: 0.000299  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "loss: 0.000297  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "loss: 0.000296  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "loss: 0.000294  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "loss: 0.000292  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "loss: 0.000291  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "loss: 0.000289  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "loss: 0.000287  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "loss: 0.000286  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "loss: 0.000284  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "loss: 0.000283  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "loss: 0.000281  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "loss: 0.000280  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "loss: 0.000278  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "loss: 0.000277  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "loss: 0.000275  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "loss: 0.000274  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "loss: 0.000272  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "loss: 0.000271  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "loss: 0.000270  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "loss: 0.000268  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "loss: 0.000267  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "loss: 0.000265  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "loss: 0.000264  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "loss: 0.000263  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "loss: 0.000261  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "loss: 0.000260  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "loss: 0.000259  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "loss: 0.000258  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "loss: 0.000256  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "loss: 0.000255  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "loss: 0.000254  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "loss: 0.000252  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "loss: 0.000251  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "loss: 0.000250  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "loss: 0.000249  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "loss: 0.000248  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "loss: 0.000246  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "loss: 0.000245  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "loss: 0.000244  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "loss: 0.000243  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "loss: 0.000242  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "loss: 0.000241  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "loss: 0.000239  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "loss: 0.000238  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "loss: 0.000237  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "loss: 0.000236  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "loss: 0.000235  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "loss: 0.000234  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "loss: 0.000233  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "loss: 0.000232  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "loss: 0.000231  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "loss: 0.000230  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "loss: 0.000229  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "loss: 0.000228  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "loss: 0.000226  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "loss: 0.000226  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "loss: 0.000224  [   64/ 1000]\n",
            "learning took 0.21 s\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "loss: 0.000224  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "loss: 0.000223  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "loss: 0.000221  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "loss: 0.000221  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "loss: 0.000220  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "loss: 0.000219  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "loss: 0.000218  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "loss: 0.000217  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "loss: 0.000216  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "loss: 0.000215  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "loss: 0.000214  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 321\n",
            "-------------------------------\n",
            "loss: 0.000213  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 322\n",
            "-------------------------------\n",
            "loss: 0.000212  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 323\n",
            "-------------------------------\n",
            "loss: 0.000211  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 324\n",
            "-------------------------------\n",
            "loss: 0.000210  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 325\n",
            "-------------------------------\n",
            "loss: 0.000209  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 326\n",
            "-------------------------------\n",
            "loss: 0.000209  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 327\n",
            "-------------------------------\n",
            "loss: 0.000208  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 328\n",
            "-------------------------------\n",
            "loss: 0.000207  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 329\n",
            "-------------------------------\n",
            "loss: 0.000206  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 330\n",
            "-------------------------------\n",
            "loss: 0.000205  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 331\n",
            "-------------------------------\n",
            "loss: 0.000204  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 332\n",
            "-------------------------------\n",
            "loss: 0.000203  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 333\n",
            "-------------------------------\n",
            "loss: 0.000203  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 334\n",
            "-------------------------------\n",
            "loss: 0.000202  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 335\n",
            "-------------------------------\n",
            "loss: 0.000201  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 336\n",
            "-------------------------------\n",
            "loss: 0.000200  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 337\n",
            "-------------------------------\n",
            "loss: 0.000199  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 338\n",
            "-------------------------------\n",
            "loss: 0.000198  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 339\n",
            "-------------------------------\n",
            "loss: 0.000198  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 340\n",
            "-------------------------------\n",
            "loss: 0.000197  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 341\n",
            "-------------------------------\n",
            "loss: 0.000196  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 342\n",
            "-------------------------------\n",
            "loss: 0.000195  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 343\n",
            "-------------------------------\n",
            "loss: 0.000195  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 344\n",
            "-------------------------------\n",
            "loss: 0.000194  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 345\n",
            "-------------------------------\n",
            "loss: 0.000193  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 346\n",
            "-------------------------------\n",
            "loss: 0.000192  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 347\n",
            "-------------------------------\n",
            "loss: 0.000191  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 348\n",
            "-------------------------------\n",
            "loss: 0.000191  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 349\n",
            "-------------------------------\n",
            "loss: 0.000190  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 350\n",
            "-------------------------------\n",
            "loss: 0.000189  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 351\n",
            "-------------------------------\n",
            "loss: 0.000188  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 352\n",
            "-------------------------------\n",
            "loss: 0.000188  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 353\n",
            "-------------------------------\n",
            "loss: 0.000187  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 354\n",
            "-------------------------------\n",
            "loss: 0.000186  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 355\n",
            "-------------------------------\n",
            "loss: 0.000186  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 356\n",
            "-------------------------------\n",
            "loss: 0.000185  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 357\n",
            "-------------------------------\n",
            "loss: 0.000184  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 358\n",
            "-------------------------------\n",
            "loss: 0.000183  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 359\n",
            "-------------------------------\n",
            "loss: 0.000183  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 360\n",
            "-------------------------------\n",
            "loss: 0.000182  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 361\n",
            "-------------------------------\n",
            "loss: 0.000181  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 362\n",
            "-------------------------------\n",
            "loss: 0.000181  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 363\n",
            "-------------------------------\n",
            "loss: 0.000180  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 364\n",
            "-------------------------------\n",
            "loss: 0.000179  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 365\n",
            "-------------------------------\n",
            "loss: 0.000179  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 366\n",
            "-------------------------------\n",
            "loss: 0.000178  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 367\n",
            "-------------------------------\n",
            "loss: 0.000177  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 368\n",
            "-------------------------------\n",
            "loss: 0.000177  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 369\n",
            "-------------------------------\n",
            "loss: 0.000176  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 370\n",
            "-------------------------------\n",
            "loss: 0.000175  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 371\n",
            "-------------------------------\n",
            "loss: 0.000175  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 372\n",
            "-------------------------------\n",
            "loss: 0.000174  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 373\n",
            "-------------------------------\n",
            "loss: 0.000174  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 374\n",
            "-------------------------------\n",
            "loss: 0.000173  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 375\n",
            "-------------------------------\n",
            "loss: 0.000172  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 376\n",
            "-------------------------------\n",
            "loss: 0.000172  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 377\n",
            "-------------------------------\n",
            "loss: 0.000171  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 378\n",
            "-------------------------------\n",
            "loss: 0.000170  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 379\n",
            "-------------------------------\n",
            "loss: 0.000170  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 380\n",
            "-------------------------------\n",
            "loss: 0.000169  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 381\n",
            "-------------------------------\n",
            "loss: 0.000169  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 382\n",
            "-------------------------------\n",
            "loss: 0.000168  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 383\n",
            "-------------------------------\n",
            "loss: 0.000167  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 384\n",
            "-------------------------------\n",
            "loss: 0.000167  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 385\n",
            "-------------------------------\n",
            "loss: 0.000166  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 386\n",
            "-------------------------------\n",
            "loss: 0.000166  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 387\n",
            "-------------------------------\n",
            "loss: 0.000165  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 388\n",
            "-------------------------------\n",
            "loss: 0.000165  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 389\n",
            "-------------------------------\n",
            "loss: 0.000164  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 390\n",
            "-------------------------------\n",
            "loss: 0.000163  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 391\n",
            "-------------------------------\n",
            "loss: 0.000163  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 392\n",
            "-------------------------------\n",
            "loss: 0.000162  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 393\n",
            "-------------------------------\n",
            "loss: 0.000162  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 394\n",
            "-------------------------------\n",
            "loss: 0.000161  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 395\n",
            "-------------------------------\n",
            "loss: 0.000161  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 396\n",
            "-------------------------------\n",
            "loss: 0.000160  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 397\n",
            "-------------------------------\n",
            "loss: 0.000160  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 398\n",
            "-------------------------------\n",
            "loss: 0.000159  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 399\n",
            "-------------------------------\n",
            "loss: 0.000158  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 400\n",
            "-------------------------------\n",
            "loss: 0.000158  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 401\n",
            "-------------------------------\n",
            "loss: 0.000157  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 402\n",
            "-------------------------------\n",
            "loss: 0.000157  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 403\n",
            "-------------------------------\n",
            "loss: 0.000156  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 404\n",
            "-------------------------------\n",
            "loss: 0.000156  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 405\n",
            "-------------------------------\n",
            "loss: 0.000155  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 406\n",
            "-------------------------------\n",
            "loss: 0.000155  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 407\n",
            "-------------------------------\n",
            "loss: 0.000154  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 408\n",
            "-------------------------------\n",
            "loss: 0.000154  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 409\n",
            "-------------------------------\n",
            "loss: 0.000153  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 410\n",
            "-------------------------------\n",
            "loss: 0.000153  [   64/ 1000]\n",
            "learning took 0.20 s\n",
            "Epoch 411\n",
            "-------------------------------\n",
            "loss: 0.000152  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 412\n",
            "-------------------------------\n",
            "loss: 0.000152  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 413\n",
            "-------------------------------\n",
            "loss: 0.000151  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 414\n",
            "-------------------------------\n",
            "loss: 0.000151  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 415\n",
            "-------------------------------\n",
            "loss: 0.000150  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 416\n",
            "-------------------------------\n",
            "loss: 0.000150  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 417\n",
            "-------------------------------\n",
            "loss: 0.000149  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 418\n",
            "-------------------------------\n",
            "loss: 0.000149  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 419\n",
            "-------------------------------\n",
            "loss: 0.000148  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 420\n",
            "-------------------------------\n",
            "loss: 0.000148  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 421\n",
            "-------------------------------\n",
            "loss: 0.000147  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 422\n",
            "-------------------------------\n",
            "loss: 0.000147  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 423\n",
            "-------------------------------\n",
            "loss: 0.000146  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 424\n",
            "-------------------------------\n",
            "loss: 0.000146  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 425\n",
            "-------------------------------\n",
            "loss: 0.000146  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 426\n",
            "-------------------------------\n",
            "loss: 0.000145  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 427\n",
            "-------------------------------\n",
            "loss: 0.000145  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 428\n",
            "-------------------------------\n",
            "loss: 0.000144  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 429\n",
            "-------------------------------\n",
            "loss: 0.000144  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 430\n",
            "-------------------------------\n",
            "loss: 0.000143  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 431\n",
            "-------------------------------\n",
            "loss: 0.000143  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 432\n",
            "-------------------------------\n",
            "loss: 0.000142  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 433\n",
            "-------------------------------\n",
            "loss: 0.000142  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 434\n",
            "-------------------------------\n",
            "loss: 0.000142  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 435\n",
            "-------------------------------\n",
            "loss: 0.000141  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 436\n",
            "-------------------------------\n",
            "loss: 0.000141  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 437\n",
            "-------------------------------\n",
            "loss: 0.000140  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 438\n",
            "-------------------------------\n",
            "loss: 0.000140  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 439\n",
            "-------------------------------\n",
            "loss: 0.000139  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 440\n",
            "-------------------------------\n",
            "loss: 0.000139  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 441\n",
            "-------------------------------\n",
            "loss: 0.000139  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 442\n",
            "-------------------------------\n",
            "loss: 0.000138  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 443\n",
            "-------------------------------\n",
            "loss: 0.000138  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 444\n",
            "-------------------------------\n",
            "loss: 0.000137  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 445\n",
            "-------------------------------\n",
            "loss: 0.000137  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 446\n",
            "-------------------------------\n",
            "loss: 0.000136  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 447\n",
            "-------------------------------\n",
            "loss: 0.000136  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 448\n",
            "-------------------------------\n",
            "loss: 0.000136  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 449\n",
            "-------------------------------\n",
            "loss: 0.000135  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 450\n",
            "-------------------------------\n",
            "loss: 0.000135  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 451\n",
            "-------------------------------\n",
            "loss: 0.000134  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 452\n",
            "-------------------------------\n",
            "loss: 0.000134  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 453\n",
            "-------------------------------\n",
            "loss: 0.000134  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 454\n",
            "-------------------------------\n",
            "loss: 0.000133  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 455\n",
            "-------------------------------\n",
            "loss: 0.000133  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 456\n",
            "-------------------------------\n",
            "loss: 0.000133  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 457\n",
            "-------------------------------\n",
            "loss: 0.000132  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 458\n",
            "-------------------------------\n",
            "loss: 0.000132  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 459\n",
            "-------------------------------\n",
            "loss: 0.000131  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 460\n",
            "-------------------------------\n",
            "loss: 0.000131  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 461\n",
            "-------------------------------\n",
            "loss: 0.000131  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 462\n",
            "-------------------------------\n",
            "loss: 0.000130  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 463\n",
            "-------------------------------\n",
            "loss: 0.000130  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 464\n",
            "-------------------------------\n",
            "loss: 0.000130  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 465\n",
            "-------------------------------\n",
            "loss: 0.000129  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 466\n",
            "-------------------------------\n",
            "loss: 0.000129  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 467\n",
            "-------------------------------\n",
            "loss: 0.000128  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 468\n",
            "-------------------------------\n",
            "loss: 0.000128  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 469\n",
            "-------------------------------\n",
            "loss: 0.000128  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 470\n",
            "-------------------------------\n",
            "loss: 0.000127  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 471\n",
            "-------------------------------\n",
            "loss: 0.000127  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 472\n",
            "-------------------------------\n",
            "loss: 0.000127  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 473\n",
            "-------------------------------\n",
            "loss: 0.000126  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 474\n",
            "-------------------------------\n",
            "loss: 0.000126  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 475\n",
            "-------------------------------\n",
            "loss: 0.000126  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 476\n",
            "-------------------------------\n",
            "loss: 0.000125  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 477\n",
            "-------------------------------\n",
            "loss: 0.000125  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 478\n",
            "-------------------------------\n",
            "loss: 0.000124  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 479\n",
            "-------------------------------\n",
            "loss: 0.000124  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 480\n",
            "-------------------------------\n",
            "loss: 0.000124  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 481\n",
            "-------------------------------\n",
            "loss: 0.000123  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 482\n",
            "-------------------------------\n",
            "loss: 0.000123  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 483\n",
            "-------------------------------\n",
            "loss: 0.000123  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 484\n",
            "-------------------------------\n",
            "loss: 0.000122  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 485\n",
            "-------------------------------\n",
            "loss: 0.000122  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 486\n",
            "-------------------------------\n",
            "loss: 0.000122  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 487\n",
            "-------------------------------\n",
            "loss: 0.000121  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 488\n",
            "-------------------------------\n",
            "loss: 0.000121  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 489\n",
            "-------------------------------\n",
            "loss: 0.000121  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 490\n",
            "-------------------------------\n",
            "loss: 0.000120  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 491\n",
            "-------------------------------\n",
            "loss: 0.000120  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 492\n",
            "-------------------------------\n",
            "loss: 0.000120  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 493\n",
            "-------------------------------\n",
            "loss: 0.000120  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 494\n",
            "-------------------------------\n",
            "loss: 0.000119  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 495\n",
            "-------------------------------\n",
            "loss: 0.000119  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 496\n",
            "-------------------------------\n",
            "loss: 0.000119  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 497\n",
            "-------------------------------\n",
            "loss: 0.000118  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 498\n",
            "-------------------------------\n",
            "loss: 0.000118  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 499\n",
            "-------------------------------\n",
            "loss: 0.000118  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 500\n",
            "-------------------------------\n",
            "loss: 0.000117  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 501\n",
            "-------------------------------\n",
            "loss: 0.000117  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 502\n",
            "-------------------------------\n",
            "loss: 0.000117  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 503\n",
            "-------------------------------\n",
            "loss: 0.000116  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 504\n",
            "-------------------------------\n",
            "loss: 0.000116  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 505\n",
            "-------------------------------\n",
            "loss: 0.000116  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 506\n",
            "-------------------------------\n",
            "loss: 0.000116  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 507\n",
            "-------------------------------\n",
            "loss: 0.000115  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 508\n",
            "-------------------------------\n",
            "loss: 0.000115  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 509\n",
            "-------------------------------\n",
            "loss: 0.000115  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 510\n",
            "-------------------------------\n",
            "loss: 0.000114  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 511\n",
            "-------------------------------\n",
            "loss: 0.000114  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 512\n",
            "-------------------------------\n",
            "loss: 0.000114  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 513\n",
            "-------------------------------\n",
            "loss: 0.000113  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 514\n",
            "-------------------------------\n",
            "loss: 0.000113  [   64/ 1000]\n",
            "learning took 0.23 s\n",
            "Epoch 515\n",
            "-------------------------------\n",
            "loss: 0.000113  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 516\n",
            "-------------------------------\n",
            "loss: 0.000113  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 517\n",
            "-------------------------------\n",
            "loss: 0.000112  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 518\n",
            "-------------------------------\n",
            "loss: 0.000112  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 519\n",
            "-------------------------------\n",
            "loss: 0.000112  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 520\n",
            "-------------------------------\n",
            "loss: 0.000111  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 521\n",
            "-------------------------------\n",
            "loss: 0.000111  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 522\n",
            "-------------------------------\n",
            "loss: 0.000111  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 523\n",
            "-------------------------------\n",
            "loss: 0.000111  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 524\n",
            "-------------------------------\n",
            "loss: 0.000110  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 525\n",
            "-------------------------------\n",
            "loss: 0.000110  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 526\n",
            "-------------------------------\n",
            "loss: 0.000110  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 527\n",
            "-------------------------------\n",
            "loss: 0.000110  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 528\n",
            "-------------------------------\n",
            "loss: 0.000109  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 529\n",
            "-------------------------------\n",
            "loss: 0.000109  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 530\n",
            "-------------------------------\n",
            "loss: 0.000109  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 531\n",
            "-------------------------------\n",
            "loss: 0.000108  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 532\n",
            "-------------------------------\n",
            "loss: 0.000108  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 533\n",
            "-------------------------------\n",
            "loss: 0.000108  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 534\n",
            "-------------------------------\n",
            "loss: 0.000108  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 535\n",
            "-------------------------------\n",
            "loss: 0.000107  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 536\n",
            "-------------------------------\n",
            "loss: 0.000107  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 537\n",
            "-------------------------------\n",
            "loss: 0.000107  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 538\n",
            "-------------------------------\n",
            "loss: 0.000107  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 539\n",
            "-------------------------------\n",
            "loss: 0.000106  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 540\n",
            "-------------------------------\n",
            "loss: 0.000106  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 541\n",
            "-------------------------------\n",
            "loss: 0.000106  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 542\n",
            "-------------------------------\n",
            "loss: 0.000106  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 543\n",
            "-------------------------------\n",
            "loss: 0.000105  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 544\n",
            "-------------------------------\n",
            "loss: 0.000105  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 545\n",
            "-------------------------------\n",
            "loss: 0.000105  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 546\n",
            "-------------------------------\n",
            "loss: 0.000105  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 547\n",
            "-------------------------------\n",
            "loss: 0.000104  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 548\n",
            "-------------------------------\n",
            "loss: 0.000104  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 549\n",
            "-------------------------------\n",
            "loss: 0.000104  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 550\n",
            "-------------------------------\n",
            "loss: 0.000104  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 551\n",
            "-------------------------------\n",
            "loss: 0.000103  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 552\n",
            "-------------------------------\n",
            "loss: 0.000103  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 553\n",
            "-------------------------------\n",
            "loss: 0.000103  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 554\n",
            "-------------------------------\n",
            "loss: 0.000103  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 555\n",
            "-------------------------------\n",
            "loss: 0.000102  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 556\n",
            "-------------------------------\n",
            "loss: 0.000102  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 557\n",
            "-------------------------------\n",
            "loss: 0.000102  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 558\n",
            "-------------------------------\n",
            "loss: 0.000102  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 559\n",
            "-------------------------------\n",
            "loss: 0.000101  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 560\n",
            "-------------------------------\n",
            "loss: 0.000101  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 561\n",
            "-------------------------------\n",
            "loss: 0.000101  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 562\n",
            "-------------------------------\n",
            "loss: 0.000101  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 563\n",
            "-------------------------------\n",
            "loss: 0.000100  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 564\n",
            "-------------------------------\n",
            "loss: 0.000100  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 565\n",
            "-------------------------------\n",
            "loss: 0.000100  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 566\n",
            "-------------------------------\n",
            "loss: 0.000100  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 567\n",
            "-------------------------------\n",
            "loss: 0.000100  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 568\n",
            "-------------------------------\n",
            "loss: 0.000099  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 569\n",
            "-------------------------------\n",
            "loss: 0.000099  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 570\n",
            "-------------------------------\n",
            "loss: 0.000099  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 571\n",
            "-------------------------------\n",
            "loss: 0.000099  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 572\n",
            "-------------------------------\n",
            "loss: 0.000098  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 573\n",
            "-------------------------------\n",
            "loss: 0.000098  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 574\n",
            "-------------------------------\n",
            "loss: 0.000098  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 575\n",
            "-------------------------------\n",
            "loss: 0.000098  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 576\n",
            "-------------------------------\n",
            "loss: 0.000098  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 577\n",
            "-------------------------------\n",
            "loss: 0.000097  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 578\n",
            "-------------------------------\n",
            "loss: 0.000097  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 579\n",
            "-------------------------------\n",
            "loss: 0.000097  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 580\n",
            "-------------------------------\n",
            "loss: 0.000097  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 581\n",
            "-------------------------------\n",
            "loss: 0.000096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 582\n",
            "-------------------------------\n",
            "loss: 0.000096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 583\n",
            "-------------------------------\n",
            "loss: 0.000096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 584\n",
            "-------------------------------\n",
            "loss: 0.000096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 585\n",
            "-------------------------------\n",
            "loss: 0.000096  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 586\n",
            "-------------------------------\n",
            "loss: 0.000095  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 587\n",
            "-------------------------------\n",
            "loss: 0.000095  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 588\n",
            "-------------------------------\n",
            "loss: 0.000095  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 589\n",
            "-------------------------------\n",
            "loss: 0.000095  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 590\n",
            "-------------------------------\n",
            "loss: 0.000095  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 591\n",
            "-------------------------------\n",
            "loss: 0.000094  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 592\n",
            "-------------------------------\n",
            "loss: 0.000094  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 593\n",
            "-------------------------------\n",
            "loss: 0.000094  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 594\n",
            "-------------------------------\n",
            "loss: 0.000094  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 595\n",
            "-------------------------------\n",
            "loss: 0.000094  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 596\n",
            "-------------------------------\n",
            "loss: 0.000093  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 597\n",
            "-------------------------------\n",
            "loss: 0.000093  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 598\n",
            "-------------------------------\n",
            "loss: 0.000093  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 599\n",
            "-------------------------------\n",
            "loss: 0.000093  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 600\n",
            "-------------------------------\n",
            "loss: 0.000093  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 601\n",
            "-------------------------------\n",
            "loss: 0.000092  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 602\n",
            "-------------------------------\n",
            "loss: 0.000092  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 603\n",
            "-------------------------------\n",
            "loss: 0.000092  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 604\n",
            "-------------------------------\n",
            "loss: 0.000092  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 605\n",
            "-------------------------------\n",
            "loss: 0.000092  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 606\n",
            "-------------------------------\n",
            "loss: 0.000091  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 607\n",
            "-------------------------------\n",
            "loss: 0.000091  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 608\n",
            "-------------------------------\n",
            "loss: 0.000091  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 609\n",
            "-------------------------------\n",
            "loss: 0.000091  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 610\n",
            "-------------------------------\n",
            "loss: 0.000091  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 611\n",
            "-------------------------------\n",
            "loss: 0.000090  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 612\n",
            "-------------------------------\n",
            "loss: 0.000090  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 613\n",
            "-------------------------------\n",
            "loss: 0.000090  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 614\n",
            "-------------------------------\n",
            "loss: 0.000090  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 615\n",
            "-------------------------------\n",
            "loss: 0.000090  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 616\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 617\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 618\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 619\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 620\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 621\n",
            "-------------------------------\n",
            "loss: 0.000089  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 622\n",
            "-------------------------------\n",
            "loss: 0.000088  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 623\n",
            "-------------------------------\n",
            "loss: 0.000088  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 624\n",
            "-------------------------------\n",
            "loss: 0.000088  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 625\n",
            "-------------------------------\n",
            "loss: 0.000088  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 626\n",
            "-------------------------------\n",
            "loss: 0.000088  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 627\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 628\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 629\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 630\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 631\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 632\n",
            "-------------------------------\n",
            "loss: 0.000087  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 633\n",
            "-------------------------------\n",
            "loss: 0.000086  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 634\n",
            "-------------------------------\n",
            "loss: 0.000086  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 635\n",
            "-------------------------------\n",
            "loss: 0.000086  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 636\n",
            "-------------------------------\n",
            "loss: 0.000086  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 637\n",
            "-------------------------------\n",
            "loss: 0.000086  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 638\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 639\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 640\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 641\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 642\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 643\n",
            "-------------------------------\n",
            "loss: 0.000085  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 644\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 645\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 646\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 647\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 648\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 649\n",
            "-------------------------------\n",
            "loss: 0.000084  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 650\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 651\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 652\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 653\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 654\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 655\n",
            "-------------------------------\n",
            "loss: 0.000083  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 656\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 657\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 658\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 659\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 660\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 661\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 662\n",
            "-------------------------------\n",
            "loss: 0.000082  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 663\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 664\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 665\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 666\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 667\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 668\n",
            "-------------------------------\n",
            "loss: 0.000081  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 669\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 670\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 671\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 672\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 673\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 674\n",
            "-------------------------------\n",
            "loss: 0.000080  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 675\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 676\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 677\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 678\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 679\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 680\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 681\n",
            "-------------------------------\n",
            "loss: 0.000079  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 682\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 683\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 684\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 685\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 686\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 687\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 688\n",
            "-------------------------------\n",
            "loss: 0.000078  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 689\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 690\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 691\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 692\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 693\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 694\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 695\n",
            "-------------------------------\n",
            "loss: 0.000077  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 696\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 697\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 698\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 699\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 700\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 701\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 702\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 703\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 704\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 705\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 706\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 707\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 708\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 709\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 710\n",
            "-------------------------------\n",
            "loss: 0.000075  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 711\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 712\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 713\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 714\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 715\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 716\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 717\n",
            "-------------------------------\n",
            "loss: 0.000074  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 718\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 719\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 720\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 721\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 722\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 723\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 724\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 725\n",
            "-------------------------------\n",
            "loss: 0.000073  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 726\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 727\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 728\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 729\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 730\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 731\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 732\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 733\n",
            "-------------------------------\n",
            "loss: 0.000072  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 734\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 735\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 736\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 737\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 738\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 739\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 740\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 741\n",
            "-------------------------------\n",
            "loss: 0.000071  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 742\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 743\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 744\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 745\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 746\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 747\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 748\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 749\n",
            "-------------------------------\n",
            "loss: 0.000070  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 750\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 751\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 752\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 753\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 754\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 755\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 756\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 757\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 758\n",
            "-------------------------------\n",
            "loss: 0.000069  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 759\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 760\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 761\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 762\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 763\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 764\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 765\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 766\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 767\n",
            "-------------------------------\n",
            "loss: 0.000068  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 768\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.21 s\n",
            "Epoch 769\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 770\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 771\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 772\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 773\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 774\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 775\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 776\n",
            "-------------------------------\n",
            "loss: 0.000067  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 777\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 778\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 779\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 780\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 781\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 782\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 783\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 784\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 785\n",
            "-------------------------------\n",
            "loss: 0.000066  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 786\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 787\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 788\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 789\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 790\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 791\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 792\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 793\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 794\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 795\n",
            "-------------------------------\n",
            "loss: 0.000065  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 796\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 797\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 798\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 799\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 800\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 801\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 802\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 803\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 804\n",
            "-------------------------------\n",
            "loss: 0.000064  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 805\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 806\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 807\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 808\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 809\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 810\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 811\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 812\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 813\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 814\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 815\n",
            "-------------------------------\n",
            "loss: 0.000063  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 816\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 817\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 818\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 819\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 820\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 821\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 822\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 823\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 824\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 825\n",
            "-------------------------------\n",
            "loss: 0.000062  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 826\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 827\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 828\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 829\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 830\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 831\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 832\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 833\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 834\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 835\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 836\n",
            "-------------------------------\n",
            "loss: 0.000061  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 837\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 838\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 839\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 840\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 841\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 842\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 843\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 844\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 845\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 846\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 847\n",
            "-------------------------------\n",
            "loss: 0.000060  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 848\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 849\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 850\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 851\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 852\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 853\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 854\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 855\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 856\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 857\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 858\n",
            "-------------------------------\n",
            "loss: 0.000059  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 859\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 860\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 861\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 862\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 863\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 864\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 865\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 866\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 867\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 868\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 869\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 870\n",
            "-------------------------------\n",
            "loss: 0.000058  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 871\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 872\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 873\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 874\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 875\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 876\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 877\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 878\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 879\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 880\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 881\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 882\n",
            "-------------------------------\n",
            "loss: 0.000057  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 883\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 884\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 885\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 886\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 887\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 888\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 889\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 890\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 891\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 892\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 893\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 894\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 895\n",
            "-------------------------------\n",
            "loss: 0.000056  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 896\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 897\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 898\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 899\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 900\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 901\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 902\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 903\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 904\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 905\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 906\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 907\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 908\n",
            "-------------------------------\n",
            "loss: 0.000055  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 909\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 910\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 911\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 912\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 913\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 914\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 915\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 916\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 917\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 918\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 919\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 920\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 921\n",
            "-------------------------------\n",
            "loss: 0.000054  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 922\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 923\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 924\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 925\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 926\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 927\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 928\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 929\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 930\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 931\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 932\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 933\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 934\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 935\n",
            "-------------------------------\n",
            "loss: 0.000053  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 936\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 937\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 938\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 939\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 940\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 941\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 942\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 943\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 944\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 945\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 946\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 947\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 948\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 949\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 950\n",
            "-------------------------------\n",
            "loss: 0.000052  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 951\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 952\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 953\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 954\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 955\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 956\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 957\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 958\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 959\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 960\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 961\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 962\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 963\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 964\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 965\n",
            "-------------------------------\n",
            "loss: 0.000051  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 966\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 967\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 968\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 969\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 970\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 971\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 972\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 973\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 974\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 975\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 976\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 977\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 978\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 979\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 980\n",
            "-------------------------------\n",
            "loss: 0.000050  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 981\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 982\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 983\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 984\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 985\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 986\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 987\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 988\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 989\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 990\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 991\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 992\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 993\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 994\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 995\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 996\n",
            "-------------------------------\n",
            "loss: 0.000049  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 997\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 998\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 999\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1000\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1001\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1002\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1003\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1004\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1005\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1006\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1007\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1008\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1009\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1010\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1011\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1012\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1013\n",
            "-------------------------------\n",
            "loss: 0.000048  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1014\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1015\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1016\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1017\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1018\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1019\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1020\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1021\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1022\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1023\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1024\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1025\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1026\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1027\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1028\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1029\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1030\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1031\n",
            "-------------------------------\n",
            "loss: 0.000047  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1032\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1033\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1034\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1035\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1036\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1037\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1038\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1039\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1040\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1041\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1042\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1043\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1044\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1045\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1046\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1047\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1048\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1049\n",
            "-------------------------------\n",
            "loss: 0.000046  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1050\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1051\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1052\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1053\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 1054\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1055\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1056\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1057\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1058\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1059\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1060\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1061\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1062\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1063\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1064\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1065\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1066\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 1067\n",
            "-------------------------------\n",
            "loss: 0.000045  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 1068\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1069\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1070\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1071\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1072\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1073\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1074\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1075\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1076\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1077\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1078\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1079\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1080\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1081\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1082\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1083\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1084\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1085\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1086\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1087\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1088\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1089\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1090\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1091\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1092\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1093\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1094\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1095\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1096\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1097\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1098\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1099\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1100\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1101\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1102\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1103\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1104\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1105\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1106\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1107\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1108\n",
            "-------------------------------\n",
            "loss: 0.000043  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1109\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.19 s\n",
            "Epoch 1110\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1111\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1112\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1113\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1114\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1115\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1116\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1117\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1118\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1119\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1120\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1121\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1122\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1123\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1124\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1125\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1126\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1127\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1128\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1129\n",
            "-------------------------------\n",
            "loss: 0.000042  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1130\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1131\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1132\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1133\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1134\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1135\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1136\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1137\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1138\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1139\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1140\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1141\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1142\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1143\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1144\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1145\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1146\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1147\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1148\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1149\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1150\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1151\n",
            "-------------------------------\n",
            "loss: 0.000041  [   64/ 1000]\n",
            "learning took 0.22 s\n",
            "Epoch 1152\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1153\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1154\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1155\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1156\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1157\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1158\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1159\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1160\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1161\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1162\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1163\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1164\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1165\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1166\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1167\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1168\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1169\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1170\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1171\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1172\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1173\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1174\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1175\n",
            "-------------------------------\n",
            "loss: 0.000040  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1176\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1177\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1178\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1179\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1180\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1181\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1182\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1183\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1184\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1185\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1186\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1187\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1188\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1189\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1190\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1191\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1192\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1193\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1194\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1195\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1196\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1197\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1198\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.17 s\n",
            "Epoch 1199\n",
            "-------------------------------\n",
            "loss: 0.000039  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Epoch 1200\n",
            "-------------------------------\n",
            "loss: 0.000038  [   64/ 1000]\n",
            "learning took 0.18 s\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train(\n",
        "      model=model, \n",
        "      epochs=1200, \n",
        "      test_dataloader=test_dataloader, \n",
        "      train_dataloader=train_dataloader, \n",
        "      optimizer=optimizer, \n",
        "      loss_fn=loss_fn,\n",
        "      output_path=OUTPUT_ROOT\n",
        "      )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
